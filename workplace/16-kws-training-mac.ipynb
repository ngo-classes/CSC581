{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Keyword Spotting\n",
    "\n",
    "## Preparation\n",
    "\n",
    "- Download and unzip [tensorflow-2.18.0](https://github.com/tensorflow/tensorflow/archive/v2.18.0.zip) source code. \n",
    "- Use the path in the cell below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import sys\n",
    "sys.path.append(\"/Users/lngo/workspace/tensorflow-2.18.0/tensorflow/examples/speech_commands/\")\n",
    "import models\n",
    "import input_data\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spotting these words: yes,no\n",
      "Training these words: yes,no\n",
      "Training steps in each stage: 12000,3000\n",
      "Learning rate in each stage: 0.001,0.0001\n",
      "Total number of training steps: 15000\n"
     ]
    }
   ],
   "source": [
    "# A comma-delimited list of the words you want to train for.\n",
    "# All the other words you do not select will be used to train \n",
    "# an \"unknown\" label so that the model does not just recognize\n",
    "# speech but your specific words. Audio data with no spoken \n",
    "# words will be used to train a \"silence\" label.\n",
    "\n",
    "WANTED_WORDS = \"yes,no\"\n",
    "TRAINING_STEPS = \"12000,3000\"\n",
    "LEARNING_RATE = \"0.001,0.0001\"\n",
    "MODEL_ARCHITECTURE = 'tiny_conv'\n",
    "# Print the configuration to confirm it\n",
    "print(\"Spotting these words: %s\" % WANTED_WORDS)\n",
    "\n",
    "# Calculate the total number of steps, which is used to identify the checkpoint\n",
    "# file name.\n",
    "TOTAL_STEPS = str(sum(map(lambda string: int(string), TRAINING_STEPS.split(\",\"))))\n",
    "\n",
    "# Print the configuration to confirm it\n",
    "print(\"Training these words: %s\" % WANTED_WORDS)\n",
    "print(\"Training steps in each stage: %s\" % TRAINING_STEPS)\n",
    "print(\"Learning rate in each stage: %s\" % LEARNING_RATE)\n",
    "print(\"Total number of training steps: %s\" % TOTAL_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of 'silence' and 'unknown' training samples required\n",
    "# to ensure that we have equal number of samples for each label.\n",
    "number_of_labels = WANTED_WORDS.count(',') + 1\n",
    "number_of_total_labels = number_of_labels + 2 # for 'silence' and 'unknown' label\n",
    "equal_percentage_of_training_samples = int(100.0/(number_of_total_labels))\n",
    "SILENT_PERCENTAGE = equal_percentage_of_training_samples\n",
    "UNKNOWN_PERCENTAGE = equal_percentage_of_training_samples\n",
    "\n",
    "# Constants which are shared during training and inference\n",
    "PREPROCESS = 'micro'\n",
    "WINDOW_STRIDE = 20\n",
    "\n",
    "# Constants used during training only\n",
    "VERBOSITY = 'DEBUG'\n",
    "EVAL_STEP_INTERVAL = '1000'\n",
    "SAVE_STEP_INTERVAL = '1000'\n",
    "\n",
    "# Constants for training directories and filepaths\n",
    "DATASET_DIR =  'dataset/'\n",
    "LOGS_DIR = 'logs/'\n",
    "TRAIN_DIR = 'train/' # for training checkpoints and other files.\n",
    "\n",
    "# Constants for inference directories and filepaths\n",
    "import os\n",
    "MODELS_DIR = 'models'\n",
    "if not os.path.exists(MODELS_DIR):\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODEL_TF = os.path.join(MODELS_DIR, 'model.pb')\n",
    "MODEL_TFLITE = os.path.join(MODELS_DIR, 'model.tflite')\n",
    "FLOAT_MODEL_TFLITE = os.path.join(MODELS_DIR, 'float_model.tflite')\n",
    "MODEL_TFLITE_MICRO = os.path.join(MODELS_DIR, 'model.cc')\n",
    "SAVED_MODEL = os.path.join(MODELS_DIR, 'saved_model')\n",
    "\n",
    "# Constants for Quantization\n",
    "QUANT_INPUT_MIN = 0.0\n",
    "QUANT_INPUT_MAX = 26.0\n",
    "QUANT_INPUT_RANGE = QUANT_INPUT_MAX - QUANT_INPUT_MIN\n",
    "\n",
    "# Constants for audio process during Quantization and Evaluation\n",
    "SAMPLE_RATE = 16000\n",
    "CLIP_DURATION_MS = 1000\n",
    "WINDOW_SIZE_MS = 30.0\n",
    "FEATURE_BIN_COUNT = 40\n",
    "BACKGROUND_FREQUENCY = 0.8\n",
    "BACKGROUND_VOLUME_RANGE = 0.1\n",
    "TIME_SHIFT_MS = 100.0\n",
    "\n",
    "# URL for the dataset and train/val/test split\n",
    "DATA_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
    "VALIDATION_PERCENTAGE = 10\n",
    "TESTING_PERCENTAGE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correct flattened input data shape for later use in model conversion\n",
    "# since the model takes a flattened version of the spectrogram. The shape is number of \n",
    "# overlapping windows times the number of frequency bins. For the default settings we have\n",
    "# 40 bins (as set above) times 49 windows (as calculated below) so the shape is (1,1960)\n",
    "def window_counter(total_samples, window_size, stride):\n",
    "  '''helper function to count the number of full-length overlapping windows'''\n",
    "  window_count = 0\n",
    "  sample_index = 0\n",
    "  while True:\n",
    "    window = range(sample_index,sample_index+stride)\n",
    "    if window.stop < total_samples:\n",
    "      window_count += 1\n",
    "    else:\n",
    "      break\n",
    "    \n",
    "    sample_index += stride\n",
    "  return window_count\n",
    "\n",
    "OVERLAPPING_WINDOWS = window_counter(CLIP_DURATION_MS, int(WINDOW_SIZE_MS), WINDOW_STRIDE)\n",
    "FLATTENED_SPECTROGRAM_SHAPE = (1, OVERLAPPING_WINDOWS * FEATURE_BIN_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /Users/lngo/workspace/tensorflow-2.18.0/tensorflow/examples/speech_commands/train.py \\\n",
    "--data_dir={DATASET_DIR} \\\n",
    "--wanted_words={WANTED_WORDS} \\\n",
    "--silence_percentage={SILENT_PERCENTAGE} \\\n",
    "--unknown_percentage={UNKNOWN_PERCENTAGE} \\\n",
    "--preprocess={PREPROCESS} \\\n",
    "--window_stride={WINDOW_STRIDE} \\\n",
    "--model_architecture={MODEL_ARCHITECTURE} \\\n",
    "--how_many_training_steps={TRAINING_STEPS} \\\n",
    "--learning_rate={LEARNING_RATE} \\\n",
    "--train_dir={TRAIN_DIR} \\\n",
    "--summaries_dir={LOGS_DIR} \\\n",
    "--verbosity={VERBOSITY} \\\n",
    "--eval_step_interval={EVAL_STEP_INTERVAL} \\\n",
    "--save_step_interval={SAVE_STEP_INTERVAL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {SAVED_MODEL}\n",
    "!python /Users/lngo/workspace/tensorflow-2.18.0/tensorflow/examples/speech_commands/freeze.py \\\n",
    "--wanted_words=$WANTED_WORDS \\\n",
    "--window_stride_ms=$WINDOW_STRIDE \\\n",
    "--preprocess=$PREPROCESS \\\n",
    "--model_architecture=$MODEL_ARCHITECTURE \\\n",
    "--start_checkpoint=$TRAIN_DIR$MODEL_ARCHITECTURE'.ckpt-'{TOTAL_STEPS} \\\n",
    "--save_format=saved_model \\\n",
    "--output_file={SAVED_MODEL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1741141895.491755 1770252 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "model_settings = models.prepare_model_settings(\n",
    "    len(input_data.prepare_words_list(WANTED_WORDS.split(','))),\n",
    "    SAMPLE_RATE, CLIP_DURATION_MS, WINDOW_SIZE_MS,\n",
    "    WINDOW_STRIDE, FEATURE_BIN_COUNT, PREPROCESS)\n",
    "audio_processor = input_data.AudioProcessor(\n",
    "    DATA_URL, DATASET_DIR,\n",
    "    SILENT_PERCENTAGE, UNKNOWN_PERCENTAGE,\n",
    "    WANTED_WORDS.split(','), VALIDATION_PERCENTAGE,\n",
    "    TESTING_PERCENTAGE, model_settings, LOGS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.Session() as sess: #replaces the above line for use with TF2.x\n",
    "  float_converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
    "  float_tflite_model = float_converter.convert()\n",
    "  float_tflite_model_size = open(FLOAT_MODEL_TFLITE, \"wb\").write(float_tflite_model)\n",
    "  print(\"Float model is %d bytes\" % float_tflite_model_size)\n",
    "\n",
    "  converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
    "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "  #converter.inference_input_type = tf.lite.constants.INT8\n",
    "  converter.inference_input_type = tf.compat.v1.lite.constants.INT8 #replaces the above line for use with TF2.x   \n",
    "  #converter.inference_output_type = tf.lite.constants.INT8\n",
    "  converter.inference_output_type = tf.compat.v1.lite.constants.INT8 #replaces the above line for use with TF2.x\n",
    "  def representative_dataset_gen():\n",
    "    for i in range(100):\n",
    "      data, _ = audio_processor.get_data(1, i*1, model_settings,\n",
    "                                         BACKGROUND_FREQUENCY, \n",
    "                                         BACKGROUND_VOLUME_RANGE,\n",
    "                                         TIME_SHIFT_MS,\n",
    "                                         'testing',\n",
    "                                         sess)\n",
    "      flattened_data = np.array(data.flatten(), dtype=np.float32).reshape(FLATTENED_SPECTROGRAM_SHAPE)\n",
    "      yield [flattened_data]\n",
    "  converter.representative_dataset = representative_dataset_gen\n",
    "  tflite_model = converter.convert()\n",
    "  tflite_model_size = open(MODEL_TFLITE, \"wb\").write(tflite_model)\n",
    "  print(\"Quantized model is %d bytes\" % tflite_model_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run inference\n",
    "def run_tflite_inference_testSet(tflite_model_path, model_type=\"Float\"):\n",
    "  #\n",
    "  # Load test data\n",
    "  #\n",
    "  np.random.seed(0) # set random seed for reproducible test results.\n",
    "  with tf.Session() as sess:\n",
    "  # with tf.compat.v1.Session() as sess: #replaces the above line for use with TF2.x\n",
    "    test_data, test_labels = audio_processor.get_data(\n",
    "        -1, 0, model_settings, BACKGROUND_FREQUENCY, BACKGROUND_VOLUME_RANGE,\n",
    "        TIME_SHIFT_MS, 'testing', sess)\n",
    "  test_data = np.expand_dims(test_data, axis=1).astype(np.float32)\n",
    "\n",
    "  #\n",
    "  # Initialize the interpreter\n",
    "  #\n",
    "  interpreter = tf.lite.Interpreter(tflite_model_path)\n",
    "  interpreter.allocate_tensors()\n",
    "  input_details = interpreter.get_input_details()[0]\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "  \n",
    "  #\n",
    "  # For quantized models, manually quantize the input data from float to integer\n",
    "  #\n",
    "  if model_type == \"Quantized\":\n",
    "    input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "    test_data = test_data / input_scale + input_zero_point\n",
    "    test_data = test_data.astype(input_details[\"dtype\"])\n",
    "\n",
    "  #\n",
    "  # Evaluate the predictions\n",
    "  #\n",
    "  correct_predictions = 0\n",
    "  for i in range(len(test_data)):\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_data[i])\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    top_prediction = output.argmax()\n",
    "    correct_predictions += (top_prediction == test_labels[i])\n",
    "\n",
    "  print('%s model accuracy is %f%% (Number of test samples=%d)' % (\n",
    "      model_type, (correct_predictions * 100) / len(test_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model accuracy is 88.834951% (Number of test samples=1236)\n",
      "Quantized model accuracy is 89.077670% (Number of test samples=1236)\n"
     ]
    }
   ],
   "source": [
    "# Compute float model accuracy\n",
    "run_tflite_inference_testSet(FLOAT_MODEL_TFLITE)\n",
    "\n",
    "# Compute quantized model accuracy\n",
    "run_tflite_inference_testSet(MODEL_TFLITE, model_type='Quantized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test using live audio recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "from io import BytesIO\n",
    "\n",
    "def record_audio_to_variable(record_seconds=5, \n",
    "                             rate=44100, \n",
    "                             chunk=1024, \n",
    "                             channels=1):\n",
    "    \"\"\"\n",
    "    Records audio from the default microphone for `record_seconds` seconds\n",
    "    and returns the audio data as WAV bytes (i.e., a complete WAV file in memory).\n",
    "    \"\"\"\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    audio_interface = pyaudio.PyAudio()\n",
    "\n",
    "    # Open the microphone stream\n",
    "    stream = audio_interface.open(\n",
    "        format=FORMAT,\n",
    "        channels=channels,\n",
    "        rate=rate,\n",
    "        input=True,\n",
    "        frames_per_buffer=chunk\n",
    "    )\n",
    "\n",
    "    print(\"Recording...\")\n",
    "    frames = []\n",
    "\n",
    "    # Capture data from the mic for the specified duration\n",
    "    for _ in range(int(rate / chunk * record_seconds)):\n",
    "        data = stream.read(chunk)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"Done recording!\")\n",
    "\n",
    "    # Stop & close the stream\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio_interface.terminate()\n",
    "\n",
    "    # Combine all frames into a single bytes object and write to an in-memory WAV\n",
    "    wav_io = BytesIO()\n",
    "    with wave.open(wav_io, 'wb') as wf:\n",
    "        wf.setnchannels(channels)\n",
    "        wf.setsampwidth(audio_interface.get_sample_size(FORMAT))\n",
    "        wf.setframerate(rate)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "    # Reset buffer pointer to start\n",
    "    wav_io.seek(0)\n",
    "\n",
    "    # Return the entire WAV file as bytes\n",
    "    return wav_io.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import read as wav_read\n",
    "import io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import librosa\n",
    "import scipy.io.wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio(seconds):\n",
    "    output = record_audio_to_variable(record_seconds=seconds)\n",
    "    riff_chunk_size = len(output) - 8\n",
    "\n",
    "    # Break up the chunk size into four bytes, held in b.\n",
    "    q = riff_chunk_size\n",
    "    b = []\n",
    "    for i in range(4):\n",
    "        q, r = divmod(q, 256)\n",
    "        b.append(r)\n",
    "\n",
    "    # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n",
    "    riff = output[:4] + bytes(b) + output[8:]\n",
    "\n",
    "    sr, audio = wav_read(io.BytesIO(riff))\n",
    "    return audio, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Done recording!\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# Say a keyword ['no' 'yes']\n",
    "audio_loud, sr_loud = get_audio(1)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:An interactive session is already active. This can cause out-of-memory errors or some other unexpected errors (due to the unpredictable timing of garbage collection) in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s). Please use `tf.Session()` if you intend to productionize.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:An interactive session is already active. This can cause out-of-memory errors or some other unexpected errors (due to the unpredictable timing of garbage collection) in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s). Please use `tf.Session()` if you intend to productionize.\n"
     ]
    }
   ],
   "source": [
    "# Helper function to run inference (on a single input this time)\n",
    "# Note: this also includes additional manual pre-processing\n",
    "TF_SESS = tf.compat.v1.InteractiveSession()\n",
    "def run_tflite_inference_singleFile(tflite_model_path, custom_audio, sr_custom_audio, model_type=\"Float\"):\n",
    "  #\n",
    "  # Preprocess the sample to get the features we pass to the model\n",
    "  #\n",
    "  # First re-sample to the needed rate (and convert to mono if needed)\n",
    "  custom_audio_resampled = librosa.resample(y = librosa.to_mono(np.float64(custom_audio)), orig_sr = sr_custom_audio, target_sr = SAMPLE_RATE)\n",
    "  # Then extract the loudest one second\n",
    "  scipy.io.wavfile.write('custom_audio.wav', SAMPLE_RATE, np.int16(custom_audio_resampled))\n",
    "  !/tmp/extract_loudest_section/gen/bin/extract_loudest_section custom_audio.wav ./trimmed\n",
    "  # Finally pass it through the TFLiteMicro preprocessor to produce the \n",
    "  # spectrogram/MFCC input that the model expects\n",
    "  custom_model_settings = models.prepare_model_settings(\n",
    "      0, SAMPLE_RATE, CLIP_DURATION_MS, WINDOW_SIZE_MS,\n",
    "      WINDOW_STRIDE, FEATURE_BIN_COUNT, PREPROCESS)\n",
    "  custom_audio_processor = input_data.AudioProcessor(None, None, 0, 0, '', 0, 0,\n",
    "                                                    model_settings, None)\n",
    "  custom_audio_preprocessed = custom_audio_processor.get_features_for_wav(\n",
    "                                        'custom_audio.wav', model_settings, TF_SESS)\n",
    "  # Reshape the output into a 1,1960 matrix as that is what the model expects\n",
    "  custom_audio_input = custom_audio_preprocessed[0].flatten()\n",
    "  test_data = np.reshape(custom_audio_input,(1,len(custom_audio_input)))\n",
    "\n",
    "  #\n",
    "  # Initialize the interpreter\n",
    "  #\n",
    "  interpreter = tf.lite.Interpreter(tflite_model_path)\n",
    "  interpreter.allocate_tensors()\n",
    "  input_details = interpreter.get_input_details()[0]\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "  #\n",
    "  # For quantized models, manually quantize the input data from float to integer\n",
    "  #\n",
    "  if model_type == \"Quantized\":\n",
    "    input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "    test_data = test_data / input_scale + input_zero_point\n",
    "    test_data = test_data.astype(input_details[\"dtype\"])\n",
    "\n",
    "  #\n",
    "  # Run the interpreter\n",
    "  #\n",
    "  interpreter.set_tensor(input_details[\"index\"], test_data)\n",
    "  interpreter.invoke()\n",
    "  output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "  top_prediction = output.argmax()\n",
    "\n",
    "  #\n",
    "  # Translate the output\n",
    "  #\n",
    "  top_prediction_str = ''\n",
    "  if top_prediction >= 2:\n",
    "    top_prediction_str = WANTED_WORDS.split(',')[top_prediction-2]\n",
    "  elif top_prediction == 0:\n",
    "    top_prediction_str = 'silence'\n",
    "  else:\n",
    "    top_prediction_str = 'unknown'\n",
    "\n",
    "  print('%s model guessed the value to be %s' % (model_type, top_prediction_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no such file or directory: /tmp/extract_loudest_section/gen/bin/extract_loudest_section\n",
      "Quantized model guessed the value to be no\n"
     ]
    }
   ],
   "source": [
    "run_tflite_inference_singleFile(MODEL_TFLITE, audio_loud, sr_loud, model_type=\"Quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
